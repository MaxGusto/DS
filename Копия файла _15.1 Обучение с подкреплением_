{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1O7PO3ko2qs87kRUvcr6hVdvVFdaN5MVy","timestamp":1574769103405},{"file_id":"16v1ZulyIGTb7bw0DD939H7aWKZ_2ZZ4i","timestamp":1574332444115},{"file_id":"12_tKJdCfQOqct6ehoeWWmBDToQg1AbAu","timestamp":1571428026561}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"SnBDolSeL618"},"source":["# **Обучение с Подкреплением**"]},{"cell_type":"markdown","metadata":{"id":"Up2dqyHR8j4x"},"source":["## Import библиотек"]},{"cell_type":"code","metadata":{"id":"ZIQoQnyUdjt4"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mhi9xGpnhEji","executionInfo":{"status":"ok","timestamp":1691561082714,"user_tz":-180,"elapsed":10203,"user":{"displayName":"Максим Густомясов","userId":"13875018182189485230"}}},"source":["import gym #загружаем \"тренажер\" Gym из платформы OpenAi, предоставляющий среду для работы над обучением с подкреплением\n","import numpy as np #импортируем библиотеку для работы с массивами данных\n","import keras #импортируем нейросетевую библиотеку\n","from tensorflow.keras.models import Model, load_model #из кераса подгружаем абстрактный класс базовой модели, метод загрузки предобученной модели\n","from tensorflow.keras.layers import Dense, Flatten, Input, Multiply, Lambda, Conv2D, MaxPooling2D, Reshape #из кераса загружаем необходимые слои для нейросети\n","from tensorflow.keras.optimizers import RMSprop #из кераса загружаем выбранный оптимизатор\n","import time #модуль для операций со временными характеристиками\n","\n","import matplotlib.pyplot as plt #импортируем библиотеку для визуализации данных\n","#\"магическая\"команда python для запуска библиотеки в ноутбуке\n","%matplotlib inline"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FWmqjSMve32l"},"source":["## Взгляд на игру"]},{"cell_type":"code","metadata":{"id":"LSs7en3kh_x0","colab":{"base_uri":"https://localhost:8080/","height":373},"executionInfo":{"status":"error","timestamp":1691561086422,"user_tz":-180,"elapsed":354,"user":{"displayName":"Максим Густомясов","userId":"13875018182189485230"}},"outputId":"26e4791d-a7ef-41cd-85c7-600d14e00538"},"source":["env = gym.make('Pong-v0') #создаем среду игры Понг средствами OpenAI Gym('env' = environment)\n","observation = env.reset() #задаем начальное состояние среды, которое наблюдает агент\n","observation.shape #взглянем на форму состояния среды\n","#увидим, что это изображение размером 210*160 с тремя RGB каналами"],"execution_count":2,"outputs":[{"output_type":"error","ename":"NameNotFound","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameNotFound\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-b489f4e7f91e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Pong-v0'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#создаем среду игры Понг средствами OpenAI Gym('env' = environment)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#задаем начальное состояние среды, которое наблюдает агент\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;31m#взглянем на форму состояния среды\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#увидим, что это изображение размером 210*160 с тремя RGB каналами\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, autoreset, new_step_api, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mspec_\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m             \u001b[0m_check_version_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"No registered env with id: {id}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36m_check_version_exists\u001b[0;34m(ns, name, version)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m     \u001b[0m_check_name_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36m_check_name_exists\u001b[0;34m(ns, name)\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0msuggestion_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Did you mean: `{suggestion[0]}`?\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msuggestion\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m     raise error.NameNotFound(\n\u001b[0m\u001b[1;32m    213\u001b[0m         \u001b[0;34mf\"Environment {name} doesn't exist{namespace_msg}. {suggestion_msg}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     )\n","\u001b[0;31mNameNotFound\u001b[0m: Environment Pong doesn't exist. "]}]},{"cell_type":"code","metadata":{"id":"yjP4qmiody-O"},"source":["# В случае ошибки с ROM, загрузить и импортировать ROMs, как показано в ссылке выше."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QUjyJkr6iDx0"},"source":["plt.imshow(observation) #вызовем визуализацию состояния среды с помощью метода imshow от matplotlib.pyplot\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KJis251BiFxo"},"source":["# В понге:\n","# 0 цифровое обозначение если ракетка 'остаётся на месте'\n","# 2 цифровое обозначение для действия ракеткой 'сдвинуться вверх'\n","# 3 цифровое обозначение для действия ракеткой 'сдвинуться вниз'\n","\n","# Проиграем вперёд игру на 30 кадров чтобы увидеть полную сцену игры\n","for i in range(100):\n","    observation, reward, done, info = env.step(0) #0 обозначает,что ничего не делаем, остаемся на месте\n","    plt.imshow(observation) #взглянем на текущее состояние среды: наш агент владеет зелёной ракеткой, появился соперник и мяч\n","    plt.show()\n","# step - такт, шаг в игре: принимает (action, действие агента), возвращает кортеж (observation, reward, done, info)\n","# observation (object) - текущее состояние среды, которое наблюдает агент(пиксели)\n","# reward (float) - награда за совершённое действие\n","# done (True or False) - обозначает завершился ли игровой эпизод (в понге до победы 21 очко)\n","# info (dict) - вспомогательная диагностическая информация, типа кол-во оставшихся жизней(неактуально для понга)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zQcIp2WlnhiI"},"source":["СЛЕДУЮЩИЙ КАДР"]},{"cell_type":"code","metadata":{"id":"wZhsLc87iQDy"},"source":["# Policy(политику, стратегию агента) будет позже задавать нейросеть.\n","# мы подадим ей разницу между новым и предыдущим кадром, чтобы отслеживалось смещение мяча/ракеток и данные были понятными\n","newObservation, reward, done, info = env.step(2) #задаем очередным шагом следующий кадр, при этом смещая ракетку вверх\n","plt.imshow(newObservation) #взглянем на следующий кадр(нам смещение едва видно, либо не видно, т.к визуально длина шага мала)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"THb6p7QilI4k"},"source":["## Предобработка данных"]},{"cell_type":"code","metadata":{"id":"NY4wrglpiXxQ"},"source":["# Зададим функцию предобработки данных для подачи в нейросеть\n","# избавимся вот всего лишнего(фон, размер и т.п), оставив на экране лишь мяч и ракетки\n","def preprocessFrames(newFrame,lastFrame): #подаем в функцию новый и предыдущий кадр\n","  nFrame = newFrame.astype(np.int32) #переводим в целочисленный тип новый кадр\n","  nFrame[nFrame==144] = 0 #удаляем цвета заднего фона в новом кадре(144 и 109 - значения каналов, дающих оранжевый цвет, зануляем)\n","  nFrame[nFrame==109] = 0 #удаляем цвета заднего фона в новом кадре(144 и 109 - значения каналов, дающих оранжевый цвет, зануляем)\n","  lFrame = lastFrame.astype(np.int32) #переводим в целочисленный тип предыдущий кадр\n","  lFrame[lFrame==144] = 0 #удаляем цвета заднего фона в новом кадре(144 и 109 - значения каналов, дающих оранжевый цвет, зануляем)\n","  lFrame[lFrame==109] = 0 #удаляем цвета заднего фона в новом кадре(144 и 109 - значения каналов, дающих оранжевый цвет, зануляем)\n","  deltaFrame = nFrame - lFrame #задаём разницу между новым и предыдущим кадром\n","  # Отрезаем верхнюю и нижнюю(неинформативные) части экрана, по 35 сверху и снизу\n","  deltaFrame = deltaFrame[35:195] #срезали края по высоте, осталась картинка 160*160\n","  # Делаем сжатие кадра в 2 раза по обеим сторонам изображения и оставляем монотонный канал\n","  deltaFrame=deltaFrame[::2,::2, 0]\n","  # Масштабирование чисел от 0 до 1\n","  maxValue = deltaFrame.max() if deltaFrame.max()> abs(deltaFrame.min()) else abs(deltaFrame.min())\n","  if maxValue != 0:\n","      deltaFrame=deltaFrame/maxValue\n","  return deltaFrame #функция вернет разницу между кадрами в оптимальном виде"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fIg0pSS3iaQO"},"source":["plt.imshow(preprocessFrames(newObservation, observation), plt.cm.gray) #выведем результат предобработки наших кадров\n","#смещение здесь - это шаг от черного пикселя к светлому"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RW5ADGTuijHU"},"source":["preprocessFrames(newObservation, observation) #выведем массив для этого состояния среды(кадра)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L0dkdm0OimmQ"},"source":["preprocessFrames(newObservation, observation).shape #выведем форму массива"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kBhBpQMWiK7V"},"source":["## Моделируем нейросеть"]},{"cell_type":"code","metadata":{"id":"u0BAA4yzGKhw"},"source":["# Cостоянием среды будет картинка 80*80, полученная вычитанием двух последовательных кадров, где по итогу все будет...\n","# ...заполнено нулями, а в местах смещения мяча либо ракетки - ненулевые значения.\n","# Далее keras'ом создадим policy, которая на основе состояния(картинки) выбирает действия.\n","# Output сети - вероятность того что нужно двигаться вверх"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2zCF5fYbin8v"},"source":["# Простая модель с двумя слоями, на 200 нейронов в скрытом слое и сигмоидой на выходе\n","inputs = Input(shape=(80,80)) #на входном слое сетки изображение 80*80\n","flattenedLayer = Flatten()(inputs) #перевели в вектор\n","fullConnected = Dense(units=200, activation='relu', use_bias=False)(flattenedLayer) #задали 200 нейронов и активацию релу\n","sigmoidOutput = Dense(1, activation='sigmoid', use_bias=False)(fullConnected) #сигмоида на выходе\n","# POLICY\n","policyNetworkModel = Model(inputs=inputs, outputs=sigmoidOutput) # собрали модель стратегии(Model - абстрактный класс базовой модели)\n","policyNetworkModel.summary() #посмотрим на модель"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2ehKWV5P4Mvn"},"source":["#### Conv2D слой"]},{"cell_type":"code","metadata":{"id":"t7xPKM4SM505"},"source":["# Простая модель с двумя слоями, на 200 нейронов в скрытом слое и сигмоидой на выходе\n","inputs = Input(shape=(80,80)) #на входном слое сетки изображение 80*80\n","layer = Reshape((80,80,1))(inputs)\n","layer = Conv2D(32, (3,3), padding='same')(layer)\n","layer = Conv2D(32, (3,3), padding='same')(layer)\n","layer = MaxPooling2D(2)(layer)\n","flattenedLayer = Flatten()(layer) #перевели в вектор\n","fullConnected = Dense(units=32, activation='relu', use_bias=False)(flattenedLayer) #задали 200 нейронов и активацию релу\n","sigmoidOutput = Dense(1, activation='sigmoid', use_bias=False)(fullConnected) #сигмоида на выходе\n","policyNetworkModel = Model(inputs=inputs, outputs=sigmoidOutput) # собрали модель стратегии(Model - абстрактный класс базовой модели)\n","policyNetworkModel.summary() #посмотрим на модель"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GEw1xGHWaCwG"},"source":["##Задаем функцию потерь в соответствии с вознаграждением"]},{"cell_type":"code","metadata":{"id":"vc8WJxR7iv9_"},"source":["# episode - одна тренировочная игра от начала партии до перезагрузки (розыгрыш эпизода до 21 очка; после победы или поражения вызываем reset)\n","episodeReward = Input(shape=(1,), name='episodeReward') #задаем награду за эпизод"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lOiOSUYxi4Au"},"source":["###########################\n","#Функция потерь в керасе имеет вид def loss(yTrue,yPred):...А так как нам нужно включить reward в loss, создаем...\n","#...функцию rewardedLoss поверх неё, чтобы на входе добавить episodeReward\n","###########################\n","def rewardedLoss(episodeReward): #задаем новую функцию потерь, принимающую episodeReward, награда\n","  def loss(yTrue,yPred):\n","    # подаём в кач-ве yTrue фактически сделанное действие(action)\n","    # если фактически сделанное действие было движением вверх - подаем 1 на yTrue, если нет то подаем 0\n","    # yPred - выход сетки(вероятность выбора движения вверх)\n","    # мы не подаём yPred в нейронку, его вычисляет керас\n","\n","    # сначала log(0) and log(1) неопределены - загоняем yPred между значениями:\n","    tmpPred = Lambda(lambda x: keras.backend.clip(x,0.05,0.95))(yPred)\n","    # вычисляем логарифм вероятности. yPred - вероятность выбора движения вверх\n","    # помним что yTrue = 1 когда фактически выбрано движение вверх, и 0 - когда вниз\n","    # формула похожа на кросс-энтропию в керасе, но здесь мы прописываем её вручную, чтобы умножить на значение награды\n","    tmpLoss = Lambda(lambda x:-yTrue*keras.backend.log(x)-(1-yTrue)*(keras.backend.log(1-x)))(tmpPred)\n","    # обновленная функция потерь - \"функция политики\"\n","    policyLoss = Multiply()([tmpLoss, episodeReward]) #добавляем в loss умножение на награду за эпизод\n","    return policyLoss #ввели обновленную функцию политики\n","  return loss # возвращаем обновленную функцию политики"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EATpSguci8Zz"},"source":["###########################\n","# Создаем керас-оптимизатор и нейронку для обучения\n","###########################\n","# episodeReward = Input(shape=(1,), name='episodeReward') #задаем награду за эпизод (задана выше)\n","policyNetworkTrain = Model(inputs=[inputs, episodeReward],outputs=sigmoidOutput) #задаем сеть с добавлением на вход награды\n","\n","myOptimizer = RMSprop(lr=0.0001) #выбрали оптимизатор с заданной скоростью обучения\n","policyNetworkTrain.compile(optimizer=myOptimizer, loss=rewardedLoss(episodeReward)) #задаем сеть с новой функцией потерь policy"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dEgR3LiLuqUn"},"source":["## Генерация игрового эпизода с участием нейросети"]},{"cell_type":"code","metadata":{"id":"4XGRbvGAjA-F"},"source":["###########################\n","# Создадим функцию, которая будет генерировать игровой эпизод с участием нейросети\n","###########################\n","#игра начнется с того, что в новом эпизоде наш агент сделает движение(\"actualAction\") вверх с вероятностью(upProbability), которую предсказала наша...\n","#...сетка policyNetwork, принявшая на вход состояние среды reshapedInput с подбитой под сеть формой.\n","#reshapedInput же получили из состояния processedNetworkInput, которое представляет собой разницу в кадрах между новым и предыдущим...\n","#...кадром, полученную функцией preprocessFrames.\n","\n","\n","def generateEpisode(policyNetwork): #подаем на вход функции модель нейросети\n","  statesList = [] #список состояний в течение эпизода, размер = (x,80,80)\n","  upDownActionList=[] #список движений в течение эпизода: вверх - 1, вниз - 0\n","  rewardsList=[] #список наград за каждое действие\n","  networkOutputList=[] #на выходе нейросети - вероятность что нужно идти вверх; собираем список из вероятности на каждом шаге\n","  env=gym.make(\"Pong-v0\") #cоздали среду\n","  observation = env.reset() #перезагрузили состояние среды\n","  newObservation = observation #получили новое состояние, которое наблюдает агент\n","  done = False #игровой эпизод активен(не завершён)\n","\n","  while done == False: # пока игровой эпизод не завершён\n","    # На вход сети будет подаваться очередное состояние - разница между кадрами.\n","    processedNetworkInput = preprocessFrames(newFrame=newObservation, lastFrame=observation) #зададим это состояние\n","    statesList.append(processedNetworkInput) #добавим в список состояний (впоследствие станет 'x'ом для входа в нейронку)\n","    reshapedInput = np.expand_dims(processedNetworkInput, axis=0) # размер 'x' - (80,80), делаем размерность (x,(1,80,80))\n","\n","    upProbability = policyNetwork.predict(reshapedInput, batch_size=1)[0][0] #задаем вероятность шага вверх\n","    networkOutputList.append(upProbability) #добавляем к списку из вероятности идти вверх на каждом шаге\n","    actualAction = np.random.choice(a=[2,3], size=1, p=[upProbability, 1-upProbability])\n","    # сделаем фактический шаг либо вверх(2) c вероятностью upProbability, либо вниз(3) с обратной вероятностью\n","\n","    if actualAction == 2: #если пошли вверх\n","      upDownActionList.append(1) # добавляем единицу в список движений в течение эпизода\n","    else: #если не пошли вверх\n","      upDownActionList.append(0) #то добавляем ноль\n","\n","    observation = newObservation # текущий newObservation записываем как старый перед тем, как сделать следующий шаг\n","    newObservation, reward, done, info = env.step(actualAction) #сделали новый шаг, получили новую награду, новое состояние\n","\n","    rewardsList.append(reward) #добавили текущую награду в список\n","\n","    if done: #если игровой эпизод закончен\n","      break #завершаем цикл\n","\n","  env.close() #закрываем текущую среду\n","  return statesList, upDownActionList, rewardsList, networkOutputList\n","#функция возвращает: список состояний в течение эпизода,список движений в течение эпизода, список наград за каждое действие,\n","#и список вероятностей того, что нужно идти вверх"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fWg_B1B3jCqL"},"source":["###########################\n","# Сгенерируем эпизод игры с необученной сетью\n","###########################\n","statesList, upDownActionList, rewardsList, networkOutputList = generateEpisode(policyNetworkModel)\n","print(\"----------------------\")\n","print(\"Количество состояний в эпизоде = \"+str(len(statesList))) #количество состояний (кадров) в эпизоде\n","print(\"Форма состояния \"+str(statesList[0].shape)) #форма каждого состояния\n","print(\"Количество наград за эпизод = \"+str(len(rewardsList))) #количество наград за эпизод(включая нулевые награды)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ox8tCAR56igu"},"source":["#### Пример генерации некольких эпизодов \\ визуализация"]},{"cell_type":"code","metadata":{"id":"Iky-630VMaqM"},"source":["# Генерируем несколько эпизодов\n","for i in range(10):\n","###########################\n","# Сгенерируем эпизод игры с необученной сетью\n","###########################\n","  statesList, upDownActionList, rewardsList, networkOutputList = generateEpisode(policyNetworkModel)\n","  print(\"----------------------\")\n","  print(\"Количество состояний в эпизоде = \"+str(len(statesList))) #количество состояний (кадров) в эпизоде\n","  print(\"Форма состояния \"+str(statesList[0].shape)) #форма каждого состояния\n","  print(\"Количество наград за эпизод = \"+str(len(rewardsList))) #количество наград за эпизод(включая нулевые награды)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TqBtkj-ujPjr"},"source":["# Взглянем на фрагмент списка из вероятности идти вверх на каждом шаге.\n","print(networkOutputList[50:70])\n","# увидим что на каждом шаге вероятность крутится около 50% - сеть пока не понимает куда лучше шагать"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I1bbRHsmjUxn"},"source":["# Взглянем на фактические действия: 1 - вверх, 0 - вниз\n","upDownActionList[50:70]\n","# Соответственно шаги также были случайными"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f4tGmjzxjWA0"},"source":["#выведем список наград(почти везде нули, кроме тех моментов когда выигрываем или проигрываем очко)\n","print(rewardsList[50:70])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EAUzdwzhjewJ"},"source":["# Взглянем на то, сколько мы раз выиграли в течение эпизода:\n","print(\"Количество выигранных очков = \"+str(len(list(filter(lambda r: r>0,rewardsList))))) #берем как длину списка, отфильтрованного по наградам,большим 0\n","print(\"Количество проигранных очков = \"+str(len(list(filter(lambda r: r<0,rewardsList))))) #берем как длину списка, отфильтрованного по наградам,меньшим 0\n","print(\"Количество нулевых наград = \"+str(len(list(filter(lambda r: r==0,rewardsList))))) #с фильтром по 0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k-dFUNGGjgQh"},"source":["# Выведем на графике полученные награды. Ненулевые награды за каждое очко будут отмечены точками.\n","plt.plot(rewardsList, '.') #точки будут наградами на графике\n","ax=plt.gca() #получим текущие оси\n","ax.grid(True) # с фоновой сеткой"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OdrN9Jy83-Tp"},"source":["## Эффективно определяем вознаграждение"]},{"cell_type":"code","metadata":{"id":"FClyJs9ZjueI"},"source":["# Награды пока присуждаются только тем движениям, в момент после которых было пропущено или забито очко. Но так мы научимся нескоро,\n","#нам нужно указать на важность действия чуть ранее, когда мы отбивали мяч ракеткой. Чтобы сетка поняла что отбивать мяч - это хорошо,\n","#распределим награду на все шаги, предшествующие забитому/пропущеному мячу, причем чем давнее был шаг, тем меньший коэффициент награды он получит.\n","\n","###########################\n","# Сформулируем функцию, которая распределит ненулевую награду для всех шагов в удачном/неудачном розыгрыше\n","###########################\n","def processRewards(rewardList): #подадим в функцию список наград\n","  rewardDecay = 0.99 #установим высший коэффициент награды для ближнего действия к победному/проигрышному действию\n","  tmpReward = 0 #создадим временную переменную для награды\n","  rewardDecayed = np.zeros_like(rewardList,dtype=np.float32) #создадим массив из нулей для нового формата наград\n","  for i in range(len(rewardList)-1,-1,-1): #будем идти в обратную сторону от награды с шагом \"-1\"\n","    if rewardList[i] == 0: #если награда нулевая\n","      tmpReward = tmpReward*rewardDecay #зададим ей коэффициент\n","      rewardDecayed[i] = tmpReward #и добавим в массив\n","    else: #иначе\n","      tmpReward = rewardList[i] #оставим награду неизменной\n","      rewardDecayed[i] = tmpReward #и введём в том же виде в массив\n","  #запустим нормализацию значений наград, что позволит сетке лучше понимать, где были хорошие и плохие шаги\n","  rewardDecayed -= np.mean(rewardDecayed) #вычтем среднее\n","  rewardDecayed /= np.std(rewardDecayed) #разделим на стандартное отклонение\n","  return rewardDecayed\n","#функция возвращает обновленный формат наград"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w8VxXQM0jwoq"},"source":["# Посмотрим на распределение наград по новой функции\n","plt.plot(processRewards(rewardsList),'-') #добавляем в график награды с учетом новой функции\n","ax = plt.gca() #получим текущие оси\n","ax.grid(True) #с фоновой сеткой\n","# новое распределение наград даст лучшее отражение эффективности действий"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hDCw_N1DGAax"},"source":["## Пример игрового эпизода с последующей тренировкой сети"]},{"cell_type":"code","metadata":{"id":"CT4iDIRJkKME"},"source":["# Сгенерируем новый игровой эпизод:\n","statesList,upDownActionList,rewardsList,networkOutputList = generateEpisode(policyNetworkModel)\n","# сыграли до 21 очка, проиграли, получили списки всех состояний среды(кадров), шагов агента, вознаграждений и предсказаний\n","\n","print(\"Количество состояний среды = \"+str(len(statesList))) #выведем количество состояний среды(кадров) в игровом эпизоде\n","print(\"Форма состояний среды = \"+str(statesList[0].shape)) #выведем форму для каждого состояния\n","print(\"Список наград  = \"+str(len(rewardsList))) #выведем список наград в игровом эпизоде(включая нулевые)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4QchFZR-7HJk"},"source":["ДАННЫЕ ДЛЯ ТРЕНИРОВКИ"]},{"cell_type":"code","metadata":{"id":"Hj0O5xrckS_r"},"source":["# Подготовим данные для тренировки\n","x = np.array(statesList) # зададим 'x' как массив состояний\n","episodeReward = np.expand_dims(processRewards(rewardsList), 1) #подобьем размер для входа в сетку\n","\n","yTmp = np.array(upDownActionList) #зададим 'y' как список движений вверх(1)/вниз(0)\n","yTrue = np.expand_dims(yTmp, 1) # скорректируем форму под сетку\n","\n","\n","print(\"Форма наград за эпизод =\", episodeReward.shape) #выведем форму наград за эпизод\n","print(\"Форма состояний среды =\", x.shape) #выведем форму состояний среды ('x' для нейросети)\n","print(\"Форма фактических движений =\", yTrue.shape) #выведем форму фактических движений ('y' для нейросети)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_nYKeGcAkYqm"},"source":["# \"Скормим\" модели новые 'x' и 'y'\n","policyNetworkTrain.fit(x=[x, episodeReward], y=yTrue)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AFGjsI6q7OpQ"},"source":["СЕРИЯ ИГРОВЫХ ЭПИЗОДОВ"]},{"cell_type":"code","metadata":{"id":"I0XSt8-Xkgnq"},"source":["###########################\n","# Теперь создадим серию игровых эпизодов\n","# Затем предобработаем данные и скормим нейросети\n","###########################\n","def generateEpisodeBatchesTraining(nBatches=10): #подаем на вход заданное количество игровых серий\n","  env = gym.make('Pong-v0') #создаем игровую среду Pong\n","  batchStateList = [] #зададим серию списков состояний\n","  batchUpDownActionList = [] #зададим серию списков движений\n","  batchRewardsList = [] #зададим серию списков наград\n","  batchNetworkOutputList = [] #зададим серию списков из вероятности идти вверх\n","  # генерим 10 игр\n","  for i in range(nBatches): #для каждой серии\n","    statesList, upDownActionList, rewardsList, networkOutputList = generateEpisode(policyNetworkModel) #сгенерируем игровой эпизод\n","    batchStateList.extend(statesList) #добавим список состояний в серию списков состояний\n","    batchNetworkOutputList.extend(networkOutputList) #добавим список вероятностей в серию списков из вероятности идти вверх\n","    batchUpDownActionList.extend(upDownActionList) #добавим список движений в серию списков движений\n","    batchRewardsList.extend(rewardsList) # добавим список наград в серию списков наград\n","\n","  episodeReward = np.expand_dims(processRewards(batchRewardsList), 1) #зададим награды и изменим форму массива с добавлением оси\n","  x = np.array(batchStateList) #сформируем массив из серии списков состояний в качестве 'x' для нейросети\n","  yTmp = np.array(batchUpDownActionList) #зададим 'y' как серию из списков движений вверх(1)/вниз(0)\n","  yTrue = np.expand_dims(yTmp, 1) #подгоним форму 'y' массива с добавлением оси\n","\n","  history = policyNetworkTrain.fit(x=[x,episodeReward], y=yTrue, epochs=5, verbose=0) #скормим нейросети серию 'x'ов и 'y'ов\n","\n","  batchLoss = history.history['loss'][-1]\n","  return batchStateList, batchUpDownActionList, batchRewardsList, batchNetworkOutputList, batchLoss\n","  #функция вернёт серию списков состояний, серию списков движений, серию списков наград, серию списков из вероятности идти вверх"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cOGjuR96Oukg"},"source":["## Достижение победы (99 эпох)"]},{"cell_type":"code","metadata":{"id":"C0K_xSfwkicT"},"source":["trainingTimes = 99 #установим количество тренировок\n","for training in range(trainingTimes): #для каждой тренировки\n","  startTime = time.time() # Время Начало тренировки\n","  statesList,upDownActionList,rewardsList,networkOutputList, batchLoss = generateEpisodeBatchesTraining(10) #сделаем по 10 игровых эпизодов(1 эпоха - 10 игр)\n","  endTime = time.time() # Время конец тренировки\n","\n","  print(\"Тренировка = \" + str((training)+1)) #выведем на экран номер очередной тренировки\n","  print(\"Время тренировки = \" + str(round(endTime - startTime))+\"сек\") #время тренировки\n","  print(\"Ошибка на тренировке = \" + str(round(batchLoss, 5))) #выведем на экран ошибку на тренировке\n","  rr=np.array(rewardsList) #сформируем все награды\n","  print(\"Выиграли очков = \"+ str(len(rr[rr>0])) + \" Проиграли очков = \" + str(len(rr[rr<0]))) #выведем на экран сумму выигранных и проигранных очков\n","  print(\"\")\n","\n","  if training % 10 == 0: #через каждые 10 эпизодов\n","    policyNetworkModel.save(\"policyNetworkModel.h5\") #сохраним модель в памяти\n","    policyNetworkModel.save(\"policyNetworkModel\" + str(training)+\".h5\") #и для удобства сохраним модель в памяти с количеством тренировок в названии\n","    with open('rewardsModelSimple.txt','a') as recordingRewards: #также в текстовом файле будем вести записи с динамикой выигранных очков\n","      recordingRewards.write(\"training = \" + str(training) + 'выигранных очков = ' + str(len(rr[rr > 0])))\n","      recordingRewards.write(\"\\n\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bE48IL6UordI"},"source":["Победы на 99 эпохах достичь не удалось. Максимально 108 очков на 86м эпизоде (тренировке)"]},{"cell_type":"markdown","metadata":{"id":"I1-2tvGqNSeP"},"source":["### 10 тренировок - количество выигранных очков?"]},{"cell_type":"code","metadata":{"id":"k-TJ5Hk0nEcF"},"source":["trainingTimes = 10 #установим количество тренировок\n","for training in range(trainingTimes): #для каждой тренировки\n","  startTime = time.time() # Время Начало тренировки\n","  statesList,upDownActionList,rewardsList,networkOutputList, batchLoss = generateEpisodeBatchesTraining(10) #сделаем по 10 игровых эпизодов(1 эпоха - 10 игр)\n","  endTime = time.time() # Время конец тренировки\n","\n","  print(\"Тренировка = \" + str((training)+1)) #выведем на экран номер очередной тренировки\n","  print(\"Время тренировки = \" + str(round(endTime - startTime))+\"сек\") #время тренировки\n","  print(\"Ошибка на тренировке = \" + str(round(batchLoss, 5))) #выведем на экран ошибку на тренировке\n","  rr=np.array(rewardsList) #сформируем все награды\n","  print(\"Выиграли очков = \"+ str(len(rr[rr>0])) + \" Проиграли очков = \" + str(len(rr[rr<0]))) #выведем на экран сумму выигранных и проигранных очков\n","  print(\"\")\n","\n","  if training % 10 == 0: #через каждые 10 эпизодов\n","    policyNetworkModel.save(\"policyNetworkModel.h5\") #сохраним модель в памяти\n","    policyNetworkModel.save(\"policyNetworkModel\" + str(training)+\".h5\") #и для удобства сохраним модель в памяти с количеством тренировок в названии\n","    with open('rewardsModelSimple.txt','a') as recordingRewards: #также в текстовом файле будем вести записи с динамикой выигранных очков\n","      recordingRewards.write(\"training = \" + str(training) + 'выигранных очков = ' + str(len(rr[rr > 0])))\n","      recordingRewards.write(\"\\n\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"69wXYuHqH-Nw"},"source":["Вывод: постепенное улучшение результатов\n"]},{"cell_type":"markdown","metadata":{"id":"RLxe5RX0NXgm"},"source":["### 20 тренировок - количество выигранных очков?"]},{"cell_type":"code","metadata":{"id":"YFZdyOMcnIPn"},"source":["trainingTimes = 10 #установим количество тренировок\n","for training in range(trainingTimes): #для каждой тренировки\n","  startTime = time.time() # Время Начало тренировки\n","  statesList,upDownActionList,rewardsList,networkOutputList, batchLoss = generateEpisodeBatchesTraining(10) #сделаем по 10 игровых эпизодов(1 эпоха - 10 игр)\n","  endTime = time.time() # Время конец тренировки\n","\n","  print(\"Тренировка = \" + str((training)+1)) #выведем на экран номер очередной тренировки\n","  print(\"Время тренировки = \" + str(round(endTime - startTime))+\"сек\") #время тренировки\n","  print(\"Ошибка на тренировке = \" + str(round(batchLoss, 5))) #выведем на экран ошибку на тренировке\n","  rr=np.array(rewardsList) #сформируем все награды\n","  print(\"Выиграли очков = \"+ str(len(rr[rr>0])) + \" Проиграли очков = \" + str(len(rr[rr<0]))) #выведем на экран сумму выигранных и проигранных очков\n","  print(\"\")\n","\n","  if training % 10 == 0: #через каждые 10 эпизодов\n","    policyNetworkModel.save(\"policyNetworkModel.h5\") #сохраним модель в памяти\n","    policyNetworkModel.save(\"policyNetworkModel\" + str(training)+\".h5\") #и для удобства сохраним модель в памяти с количеством тренировок в названии\n","    with open('rewardsModelSimple.txt','a') as recordingRewards: #также в текстовом файле будем вести записи с динамикой выигранных очков\n","      recordingRewards.write(\"training = \" + str(training) + 'выигранных очков = ' + str(len(rr[rr > 0])))\n","      recordingRewards.write(\"\\n\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fX8FVF_kH4cA"},"source":["Вывод: 50 забитых очков. Прогресс"]},{"cell_type":"markdown","metadata":{"id":"1q97oeucNYzQ"},"source":["### 30 тренировок - количество выигранных очков?"]},{"cell_type":"code","metadata":{"id":"jAhcoa4hnJIZ"},"source":["trainingTimes = 10 #установим количество тренировок\n","for training in range(trainingTimes): #для каждой тренировки\n","  startTime = time.time() # Время Начало тренировки\n","  statesList,upDownActionList,rewardsList,networkOutputList, batchLoss = generateEpisodeBatchesTraining(10) #сделаем по 10 игровых эпизодов(1 эпоха - 10 игр)\n","  endTime = time.time() # Время конец тренировки\n","\n","  print(\"Тренировка = \" + str((training)+1)) #выведем на экран номер очередной тренировки\n","  print(\"Время тренировки = \" + str(round(endTime - startTime))+\"сек\") #время тренировки\n","  print(\"Ошибка на тренировке = \" + str(round(batchLoss, 5))) #выведем на экран ошибку на тренировке\n","  rr=np.array(rewardsList) #сформируем все награды\n","  print(\"Выиграли очков = \"+ str(len(rr[rr>0])) + \" Проиграли очков = \" + str(len(rr[rr<0]))) #выведем на экран сумму выигранных и проигранных очков\n","  print(\"\")\n","\n","  if training % 10 == 0: #через каждые 10 эпизодов\n","    policyNetworkModel.save(\"policyNetworkModel.h5\") #сохраним модель в памяти\n","    policyNetworkModel.save(\"policyNetworkModel\" + str(training)+\".h5\") #и для удобства сохраним модель в памяти с количеством тренировок в названии\n","    with open('rewardsModelSimple.txt','a') as recordingRewards: #также в текстовом файле будем вести записи с динамикой выигранных очков\n","      recordingRewards.write(\"training = \" + str(training) + 'выигранных очков = ' + str(len(rr[rr > 0])))\n","      recordingRewards.write(\"\\n\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jNWUM4KSHvdt"},"source":["Вывод: на 30 тренировке заметный прогресс -  60 забитых очков.\n","Побед нет."]},{"cell_type":"markdown","metadata":{"id":"AajzftiKybYf"},"source":["## --------------------\n","Сыграем эпизод обученной моделью и воспроизведем видео игры"]},{"cell_type":"code","metadata":{"id":"uchGOlfyyXwl"},"source":["###########################\n","# Импорт библиотек для записи и воспроизведения видео\n","###########################\n","!pip install pyvirtualdisplay > /dev/null 2>&1\n","# Устанавливаем виртуальный дисплей pyvirtualdisplay; \"/dev/null 2>&1\" уберёт длинный вывод в строке output\n","!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1 #создает вирт.дисплей в памяти(нужен для работы дисплея в ноутбуке)\n","# \"apt-get...\" - установка пакета программного обеспечения\n","# \"...-y...\" автоматический \"yes\" на вопрос \"вы действительно хотите это установить?\"\n","# \"...xvfb...\" - бэкенд для pyvirtualdisplay - сервер, который выполняет все графические операции в памяти без вывода на экран\n","# \"...python-opengl...\" - поддержка графики с помощью графической библиотеки\n","# \"...ffmpeg\" - пакеты для обработки/конвертирования видеофайла из одного формата в другой\n","\n","from gym.wrappers import Monitor #класс Monitor из пакета функций-обёрток в gym активирует видеозапись игры\n","import glob #модуль возвращает список путей к видео по шаблону (для удобства поиска/воспроизведения текущего видео)\n","import base64 #библиотека поможет нам закодировать видео в 64-разрядный код, и без повреждений/изменений открыть его в ноутбуке через HTML\n","\n","from IPython.display import HTML #загружаем модуль чтобы обратиться к HTML для открытия закодированного видео\n","from IPython import display as ipythondisplay #активирует дисплей для воспроизведения видео в интерфейсе ноутбука\n","from pyvirtualdisplay import Display #модуль для активации виртуального дисплея\n","display = Display(visible=0, size=(1400, 900)) #запустим невидимый виртуальный дисплей\n","display.start()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BSA9O3aCynx9"},"source":["###########################\n","# Задаем функции записи и воспроизведения видео\n","###########################\n","def wrapEnv(env): #зададим функцию-обёртку над средой\n","  env = Monitor(env, './video', force=True) # класс Monitor будет записывать игру в видеофайл\n","  return env #теперь вызов среды через функцию wrapEnv будет запускать видеозапись\n","\n","def showVideo(): #функция для воспроизведения видео в ноутбуке\n","  mp4list = glob.glob('video/*.mp4') #возвращает список путей к видео по этому шаблону\n","  if len(mp4list) > 0: #если по этому пути нашелся хотя бы один файл\n","    mp4 = mp4list[0] #то берём самый свежий файл\n","    video = open(mp4, 'r+b').read() #открываем в режиме чтения/записи бинарного файла\n","    encoded = base64.b64encode(video) #кодируем видео в 64-разрядный код\n","    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay\n","                loop controls style=\"height: 400px;\">\n","                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n","             </video>'''.format(encoded.decode('ascii'))))\n","    # Декодируем ascii формат в видео mp4 - \"format(encoded.decode('ascii'))))\"\n","    # для воспроизведения на разных браузерах - \"<source src=\"data:video/mp4\"\n","    # задаём высоту экрана 400px - \"height: 400px;\"\n","    # активируем элементы управления в видео(типа кнопки паузы и т.п) - \"controls\"\n","    # включаем автоповтор после окончания воспроизведения - \"loop\"\n","    # видео автоматически воспроизведется по готовности - \"autoplay\"\n","    # текстовая альтернатива описывающая объект в случае невозможности отображения - \"alt=\"test\"\"\n","\n","  else: #если путь не нашелся\n","    print(\"Could not find video\") #то выведем на печать, что не смог найти видео"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N4q1cWKjtutp"},"source":["###########################\n","# Задаем функцию сыграть и показать игру\n","###########################\n","def playAndShowEpisode(policyNetwork): #подаем на вход функции модель нейросети\n","    env = wrapEnv(gym.make('Pong-v0')) #создаем среду в режиме записи\n","    done = False #игровой эпизод не завершён\n","    observation = env.reset() #задаем начальное состояние среды, которое наблюдает агент\n","    newObservation = observation #задали новое состояние которое наблюдает агент\n","    while done == False: #пока игровой эпизод не завершён\n","        # На вход сети будет подаваться очередное состояние - разница между кадрами.\n","        processedNetworkInput = preprocessFrames(newFrame=newObservation,lastFrame=observation) #зададим это состояние\n","        reshapedInput = np.expand_dims(processedNetworkInput,axis=0) #размер 'x' - (80,80), делаем размерность (x,(1,80,80))\n","\n","        upProbability = policyNetwork.predict(reshapedInput,batch_size=1)[0][0] #задаем вероятность шага вверх\n","        actualAction = np.random.choice(a=[2,3], size=1, p=[upProbability,1-upProbability])\n","        # сделаем фактический шаг либо вверх(2) c вероятностью upProbability, либо вниз(3) с обратной вероятностью\n","\n","        env.render() #запускаем воспроизведение среды\n","\n","        observation = newObservation #текущий newObservation записываем как старый, перед тем как сделать следующий шаг\n","        newObservation, reward, done, info = env.step(actualAction) #сделали новый шаг, получили новую награду, новое состояние\n","\n","    env.close() #закрываем игровую среду после окончания игры\n","    showVideo() #показать видео игры"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3UlPhDJbtxps"},"source":["playAndShowEpisode(policyNetworkModel) #запускаем игровой эпизод c необученной моделью нейросети"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9xZOI-DXM0Lf"},"source":["#взглянем как сыграет сетка, обученная на 30 тренировках\n","policyNetworkModel30 = load_model(\"policyNetworkModel30.h5\") #подгружаем модель из сохранённых\n","playAndShowEpisode(policyNetworkModel30) #запускаем игровой эпизод"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O9S7aXBoMsOt"},"source":["#взглянем как сыграет сетка, обученная на 60 тренировках\n","policyNetworkModel60 = load_model(\"policyNetworkModel60.h5\") #подгружаем модель из сохранённых\n","playAndShowEpisode(policyNetworkModel60) #запускаем игровой эпизод"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gw8V8TK7ikPF"},"source":["#взглянем как сыграет сетка, обученная на 120 тренировках\n","policyNetworkModel120 = load_model(\"policyNetworkModel120.h5\") #подгружаем модель из сохранённых\n","playAndShowEpisode(policyNetworkModel120) #запускаем игровой эпизод"],"execution_count":null,"outputs":[]}]}